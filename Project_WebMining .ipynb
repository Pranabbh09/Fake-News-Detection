{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qFcf7DqEqK5u"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vud-yyCYphF9"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-1-d5df0069828e\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--\u003e 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5386,"status":"ok","timestamp":1720509663263,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"ke6PuLeXUTC6","outputId":"512bd0d3-d1bc-430a-b28c-03d4d1c55678"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                                   title  \\\n","0       Donald Trump Sends Out Embarrassing New Year’...   \n","1       Drunk Bragging Trump Staffer Started Russian ...   \n","2       Sheriff David Clarke Becomes An Internet Joke...   \n","3       Trump Is So Obsessed He Even Has Obama’s Name...   \n","4       Pope Francis Just Called Out Donald Trump Dur...   \n","...                                                  ...   \n","44893  'Fully committed' NATO backs new U.S. approach...   \n","44894  LexisNexis withdrew two products from Chinese ...   \n","44895  Minsk cultural hub becomes haven from authorities   \n","44896  Vatican upbeat on possibility of Pope Francis ...   \n","44897  Indonesia to buy $1.14 billion worth of Russia...   \n","\n","                                                    text    subject  \\\n","0      Donald Trump just couldn t wish all Americans ...       News   \n","1      House Intelligence Committee Chairman Devin Nu...       News   \n","2      On Friday, it was revealed that former Milwauk...       News   \n","3      On Christmas day, Donald Trump announced that ...       News   \n","4      Pope Francis used his annual Christmas Day mes...       News   \n","...                                                  ...        ...   \n","44893  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n","44894  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n","44895  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n","44896  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n","44897  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n","\n","                    date target  \n","0      December 31, 2017   fake  \n","1      December 31, 2017   fake  \n","2      December 30, 2017   fake  \n","3      December 29, 2017   fake  \n","4      December 25, 2017   fake  \n","...                  ...    ...  \n","44893   August 22, 2017    true  \n","44894   August 22, 2017    true  \n","44895   August 22, 2017    true  \n","44896   August 22, 2017    true  \n","44897   August 22, 2017    true  \n","\n","[44898 rows x 5 columns]\n"]}],"source":["# Project on Fake News Detection\n","# Using two csv files one containing real news data and other containing Fake news data. These files are created by scraping data from websites .\n","\n","\n","import pandas as pd\n","\n","fake = pd.read_csv(\"/content/drive/MyDrive/FAKE NEWS DETECTION PROJECT/Fake.csv\")\n","true = pd.read_csv(\"/content/drive/MyDrive/FAKE NEWS DETECTION PROJECT/True.csv\")\n","\n","fake['target'] = 'fake'\n","true['target'] = 'true'\n","\n","data = pd.concat([fake, true]).reset_index(drop = True)\n","print(data)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UIERZoHHUTDA"},"outputs":[],"source":["from sklearn.utils import shuffle\n","data = shuffle(data)\n","data = data.reset_index(drop=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1720509663263,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"oq84a8gOsRzy","outputId":"7633aa1f-d843-4717-e520-74eea0c54daa"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: 'Content' column not found in DataFrame.\n"]}],"source":["# Content preprocessing - converting to lower case and removing punctuation\n","\n","import string\n","\n","# Check if 'Content' column exists in data\n","if 'Content' in data.columns:\n","    # Convert 'Content' column to string and lowercase\n","    data['Content'] = data['Content'].apply(lambda x: str(x))\n","    data['Content'] = data['Content'].apply(lambda x: x.lower())\n","\n","    # Define function to remove punctuation\n","    def punctuation_removal(text):\n","        all_list = [char for char in text if char not in string.punctuation]\n","        clean_str = ''.join(all_list)\n","        return clean_str\n","\n","    # Apply punctuation removal function to 'Content' column\n","    data['Content'] = data['Content'].apply(punctuation_removal)\n","else:\n","    print(\"Error: 'Content' column not found in DataFrame.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1720509663263,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"15wyMA_JsmPa","outputId":"c5dbd2c1-3cf0-4605-c0cf-44daac6d76c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: 'Content' column not found in DataFrame.\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Stopword removal\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","\n","# Check if 'Content' column exists in data\n","if 'Content' in data.columns:\n","    # Load English stopwords\n","    stop = stopwords.words('english')\n","\n","    # Define function to remove stopwords\n","    def remove_stopwords(text):\n","        # Split text into words and remove stopwords\n","        words = [word for word in text.split() if word.lower() not in stop]\n","        # Join the remaining words back into a string\n","        return ' '.join(words)\n","\n","    # Apply stopword removal function to 'Content' column\n","    data['Content'] = data['Content'].apply(remove_stopwords)\n","else:\n","    print(\"Error: 'Content' column not found in DataFrame.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1720509663263,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"iLionNvQwK2h","outputId":"c39be4c9-6996-4aeb-e251-6ba1a10c22c2"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]}],"source":["# Stopword removal\n","import nltk\n","nltk.download('stopwords')\n","\n","from nltk.corpus import stopwords\n","\n","# Ensure 'Content' column exists in data\n","if 'Content' in data.columns:\n","    # Load English stopwords\n","    stop = stopwords.words('english')\n","\n","    # Define function to remove stopwords\n","    def remove_stopwords(text):\n","        # Split text into words and remove stopwords\n","        words = [word for word in text.split() if word.lower() not in stop]\n","        # Join the remaining words back into a string\n","        return ' '.join(words)\n","\n","    # Apply stopword removal function to 'Content\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1720509663263,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"xp_spgNJwT_w","outputId":"08cdfe46-a610-470f-ee2c-6c85765a3af0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: 'Content' or 'target' column not found in DataFrame.\n"]}],"source":["# Ensure that 'Content' and 'target' columns exist in data\n","if 'Content' in data.columns and 'target' in data.columns:\n","    from matplotlib import pyplot as plt\n","\n","    # Print the count of content for each target category\n","    print(data.groupby(['target'])['Content'].count())\n","\n","    # Plot the count of content for each target category as a bar plot\n","    data.groupby(['target'])['Content'].count().plot(kind=\"bar\")\n","    plt.show()\n","else:\n","    print(\"Error: 'Content' or 'target' column not found in DataFrame.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1720509663263,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"dpPlYYqywg55","outputId":"ca1405ec-4a88-4360-bf85-8aa128dfaa76"},"outputs":[{"name":"stdout","output_type":"stream","text":["Error: 'Content' or 'target' column not found in DataFrame.\n"]}],"source":["# Ensure that 'Content' and 'target' columns exist in data\n","if 'Content' in data.columns and 'target' in data.columns:\n","    from matplotlib import pyplot as plt\n","\n","    # Print the count of content for each target category\n","    print(data.groupby(['target'])['Content'].count())\n","\n","    # Plot the count of content for each target category as a bar plot\n","    data.groupby(['target'])['Content'].count().plot(kind=\"bar\")\n","    plt.show()\n","else:\n","    print(\"Error: 'Content' or 'target' column not found in DataFrame.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"elapsed":1429,"status":"error","timestamp":1720509664690,"user":{"displayName":"KHEER SAGAR PATEL","userId":"12499357872323884533"},"user_tz":420},"id":"dB3FJqbkUTDF","outputId":"5dd53d1f-3eed-43ae-bef8-2fd7637bd797"},"outputs":[{"ename":"KeyError","evalue":"'Column not found: Content'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-14-46dfabeac545\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 2\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1769\u001b[0m                 \u001b[0;34m\"Use a list instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1770\u001b[0m             )\n\u001b[0;32m-\u003e 1771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 244\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Column not found: {key}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gotitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'Column not found: Content'"]}],"source":["\n","from matplotlib import pyplot as plt\n","print(data.groupby(['target'])['Content'].count())\n","data.groupby(['target'])['Content'].count().plot(kind=\"bar\")\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ih0PUv3KUTDG"},"outputs":[],"source":["# Tokenization and word frequency calcultion and plotting word frequencies od Fake and Real news words\n","\n","from nltk import tokenize\n","import seaborn as sns\n","token_space = tokenize.WhitespaceTokenizer()\n","def counter(text, column_text, quantity):\n","    all_words = ' '.join([text for text in text[column_text]])\n","    token_phrase = token_space.tokenize(all_words)\n","    frequency = nltk.FreqDist(token_phrase)\n","    df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n","                                   \"Frequency\": list(frequency.values())})\n","    df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = quantity)\n","    plt.figure(figsize=(12,8))\n","    ax = sns.barplot(data = df_frequency, x = \"Word\", y = \"Frequency\", color = 'blue')\n","    ax.set(ylabel = \"Count\")\n","    plt.xticks(rotation='vertical')\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KxkEPutHUTDH"},"outputs":[],"source":["counter(data[data[\"target\"] == \"fake\"], \"Content\", 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMT-ia73UTDJ"},"outputs":[],"source":["counter(data[data[\"target\"] == \"true\"], \"Content\", 20)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxBKpEa1UTDK"},"outputs":[],"source":["# Code to plot COnfusion Matrix\n","from sklearn import metrics\n","import itertools\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","        thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] \u003e thresh else \"black\")\n","        plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IvM8OgiUTDK"},"outputs":[],"source":["# Splitting data to Train and Test in order to  fit the machine learning model\n","\n","from sklearn.model_selection import train_test_split\n","X_train,X_test,y_train,y_test = train_test_split(data['Content'], data.target, test_size=0.2, random_state=42)\n","X_test\n","y_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IbMvO1jSUTDL"},"outputs":[],"source":["# Vectorizing and applying TF-IDF Transformer and appling Logistic Regression\n","\n","from imblearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.metrics import accuracy_score\n","pipe = Pipeline([('vect', CountVectorizer()),\n","                 ('tfidf', TfidfTransformer()),\n","                 ('model', LogisticRegression())])# Fitting the model\n","model = pipe.fit(X_train, y_train)# Accuracy\n","prediction = model.predict(X_test)\n","print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qLSFufs2UTDL"},"outputs":[],"source":["\n","\n","import numpy as np\n","cm = metrics.confusion_matrix(y_test, prediction)\n","plot_confusion_matrix(cm, classes=['Fake', 'Real'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvKhUBOeUTDM"},"outputs":[],"source":["# Vectorizing and applying TF-IDF and applying DecisionTree Classifier\n","\n","from sklearn.tree import DecisionTreeClassifier\n","pipe = Pipeline([('vect', CountVectorizer()),\n","                 ('tfidf', TfidfTransformer()),\n","                 ('model', DecisionTreeClassifier(criterion= 'entropy',\n","                                           max_depth = 20,\n","                                           splitter='best',\n","                                           random_state=42))])\n","# Fitting the model\n","model = pipe.fit(X_train, y_train)# Accuracy\n","prediction = model.predict(X_test)\n","print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Z1ymJItUTDM"},"outputs":[],"source":["cm = metrics.confusion_matrix(y_test, prediction)\n","plot_confusion_matrix(cm, classes=['Fake', 'Real'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fu7Cv-nqUTDN"},"outputs":[],"source":["# Vectorizing and applying TF-IDF and applying RandomForest Classifier\n","\n","from sklearn.ensemble import RandomForestClassifier\n","pipe = Pipeline([('vect', CountVectorizer()),\n","                 ('tfidf', TfidfTransformer()),\n","                 ('model', RandomForestClassifier(n_estimators=50, criterion=\"entropy\"))])\n","model = pipe.fit(X_train, y_train)\n","prediction = model.predict(X_test)\n","print(\"accuracy: {}%\".format(round(accuracy_score(y_test, prediction)*100,2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMxJRsSVUTDN"},"outputs":[],"source":["cm = metrics.confusion_matrix(y_test, prediction)\n","plot_confusion_matrix(cm, classes=['Fake', 'Real'])"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}